{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flower Classification - EDA & Model Training\n",
    "\n",
    "This notebook contains:\n",
    "1. **Exploratory Data Analysis (EDA)** - Understanding the flowers dataset\n",
    "2. **Data Preparation** - Loading, preprocessing, augmentation\n",
    "3. **Model Training** - Baseline CNN and Transfer Learning with MobileNetV2\n",
    "4. **Hyperparameter Tuning** - Finding optimal settings\n",
    "5. **Evaluation** - Accuracy, confusion matrix, sample predictions\n",
    "\n",
    "**Dataset:** [TensorFlow Flowers](http://download.tensorflow.org/example_images/flower_photos.tgz)  \n",
    "**Classes:** Daisy, Dandelion, Rose, Sunflower, Tulip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path(\"../data/flower_photos\")\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "\n",
    "# Check if data exists\n",
    "if not DATA_DIR.exists():\n",
    "    print(\"Dataset not found! Run the download script first:\")\n",
    "    print(\"  python src/download_data.py\")\n",
    "else:\n",
    "    print(f\"Dataset found at: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class directories\n",
    "class_dirs = [d for d in DATA_DIR.iterdir() if d.is_dir()]\n",
    "class_names = sorted([d.name for d in class_dirs])\n",
    "\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images per class\n",
    "class_counts = {}\n",
    "for class_dir in class_dirs:\n",
    "    images = list(class_dir.glob(\"*.jpg\"))\n",
    "    class_counts[class_dir.name] = len(images)\n",
    "\n",
    "# Create DataFrame\n",
    "df_counts = pd.DataFrame({\n",
    "    \"class\": list(class_counts.keys()),\n",
    "    \"count\": list(class_counts.values())\n",
    "}).sort_values(\"class\")\n",
    "\n",
    "total_images = df_counts[\"count\"].sum()\n",
    "print(f\"\\nTotal images: {total_images}\")\n",
    "print(\"\\nImages per class:\")\n",
    "print(df_counts.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = sns.color_palette(\"husl\", len(class_names))\n",
    "\n",
    "bars = ax.bar(df_counts[\"class\"], df_counts[\"count\"], color=colors)\n",
    "ax.set_xlabel(\"Flower Class\")\n",
    "ax.set_ylabel(\"Number of Images\")\n",
    "ax.set_title(\"Class Distribution in Flowers Dataset\")\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, df_counts[\"count\"]):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "            str(count), ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = df_counts[\"count\"].max() / df_counts[\"count\"].min()\n",
    "print(f\"\\nClass imbalance ratio (max/min): {imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample images from each class\n",
    "fig, axes = plt.subplots(len(class_names), 5, figsize=(15, 3*len(class_names)))\n",
    "\n",
    "for i, class_name in enumerate(sorted(class_names)):\n",
    "    class_path = DATA_DIR / class_name\n",
    "    images = list(class_path.glob(\"*.jpg\"))[:5]\n",
    "    \n",
    "    for j, img_path in enumerate(images):\n",
    "        img = plt.imread(img_path)\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis(\"off\")\n",
    "        if j == 0:\n",
    "            axes[i, j].set_title(class_name.upper(), fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\"Sample Images from Each Class\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Image Properties Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze image sizes\n",
    "from PIL import Image\n",
    "\n",
    "widths, heights = [], []\n",
    "aspect_ratios = []\n",
    "\n",
    "for class_dir in class_dirs:\n",
    "    for img_path in list(class_dir.glob(\"*.jpg\"))[:100]:  # Sample 100 per class\n",
    "        with Image.open(img_path) as img:\n",
    "            w, h = img.size\n",
    "            widths.append(w)\n",
    "            heights.append(h)\n",
    "            aspect_ratios.append(w / h)\n",
    "\n",
    "print(\"Image Size Statistics:\")\n",
    "print(f\"  Width  - Min: {min(widths)}, Max: {max(widths)}, Mean: {np.mean(widths):.0f}\")\n",
    "print(f\"  Height - Min: {min(heights)}, Max: {max(heights)}, Mean: {np.mean(heights):.0f}\")\n",
    "print(f\"  Aspect Ratio - Mean: {np.mean(aspect_ratios):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot image size distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(widths, bins=30, alpha=0.7, label=\"Width\", color=\"steelblue\")\n",
    "axes[0].hist(heights, bins=30, alpha=0.7, label=\"Height\", color=\"coral\")\n",
    "axes[0].set_xlabel(\"Pixels\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_title(\"Image Dimensions Distribution\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(widths, heights, alpha=0.3, s=10)\n",
    "axes[1].set_xlabel(\"Width (pixels)\")\n",
    "axes[1].set_ylabel(\"Height (pixels)\")\n",
    "axes[1].set_title(\"Width vs Height\")\n",
    "axes[1].axhline(y=224, color='r', linestyle='--', label='Target size (224)')\n",
    "axes[1].axvline(x=224, color='r', linestyle='--')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 EDA Summary\n",
    "\n",
    "**Key Findings:**\n",
    "- TODO: Add findings after running EDA\n",
    "- Class distribution\n",
    "- Image quality observations\n",
    "- Potential challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize dataset performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomZoom(0.1),\n",
    "])\n",
    "\n",
    "# Visualize augmentation\n",
    "for images, _ in train_ds.take(1):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    \n",
    "    # Original image\n",
    "    original = images[0]\n",
    "    axes[0, 0].imshow(original.numpy().astype(\"uint8\"))\n",
    "    axes[0, 0].set_title(\"Original\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "    \n",
    "    # Augmented versions\n",
    "    for i in range(1, 8):\n",
    "        augmented = data_augmentation(tf.expand_dims(original, 0))\n",
    "        row, col = divmod(i, 4)\n",
    "        axes[row, col].imshow(augmented[0].numpy().astype(\"uint8\"))\n",
    "        axes[row, col].set_title(f\"Augmented {i}\")\n",
    "        axes[row, col].axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(\"Data Augmentation Examples\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Baseline Model (Simple CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline CNN\n",
    "baseline_model = keras.Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=IMG_SIZE + (3,)),\n",
    "    \n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    \n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    \n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "baseline_history = baseline_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Transfer Learning (MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2\n",
    "base_model = keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SIZE + (3,),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "# Freeze base model\n",
    "base_model.trainable = False\n",
    "\n",
    "print(f\"Base model layers: {len(base_model.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build transfer learning model\n",
    "preprocess_input = keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "inputs = keras.Input(shape=IMG_SIZE + (3,))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "transfer_model = keras.Model(inputs, outputs)\n",
    "\n",
    "transfer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train transfer learning model\n",
    "transfer_history = transfer_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze top layers of base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze all layers except the last 30\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "transfer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Trainable layers: {sum([1 for l in transfer_model.layers if l.trainable])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune\n",
    "fine_tune_history = transfer_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Comparison & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history comparison\n",
    "def plot_history(history, title):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Train')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title(f'{title} - Accuracy')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Loss\n",
    "    axes[1].plot(history.history['loss'], label='Train')\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title(f'{title} - Loss')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(baseline_history, \"Baseline CNN\")\n",
    "plot_history(transfer_history, \"MobileNetV2 Transfer Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"Final Evaluation on Validation Set:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "baseline_loss, baseline_acc = baseline_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"Baseline CNN:         {baseline_acc:.2%}\")\n",
    "\n",
    "transfer_loss, transfer_acc = transfer_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"MobileNetV2 (tuned):  {transfer_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in val_ds:\n",
    "    predictions = transfer_model.predict(images, verbose=0)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (Normalized)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = MODEL_DIR / \"flower_classifier.keras\"\n",
    "transfer_model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save class names\n",
    "classes_path = MODEL_DIR / \"class_names.txt\"\n",
    "with open(classes_path, 'w') as f:\n",
    "    for name in class_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "print(f\"Class names saved to: {classes_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "for images, labels in val_ds.take(1):\n",
    "    predictions = transfer_model.predict(images, verbose=0)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(12):\n",
    "        pred_idx = np.argmax(predictions[i])\n",
    "        pred_class = class_names[pred_idx]\n",
    "        true_class = class_names[labels[i]]\n",
    "        confidence = predictions[i][pred_idx]\n",
    "        \n",
    "        color = 'green' if pred_class == true_class else 'red'\n",
    "        \n",
    "        axes[i].imshow(images[i].numpy().astype('uint8'))\n",
    "        axes[i].set_title(f\"Pred: {pred_class} ({confidence:.1%})\\nTrue: {true_class}\",\n",
    "                          color=color, fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Sample Predictions (Green=Correct, Red=Wrong)\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Model | Validation Accuracy | Notes |\n",
    "|-------|---------------------|-------|\n",
    "| Baseline CNN | ~X% | Simple architecture |\n",
    "| MobileNetV2 (frozen) | ~X% | Feature extraction only |\n",
    "| MobileNetV2 (fine-tuned) | ~X% | Best performance |\n",
    "\n",
    "**Selected Model:** MobileNetV2 with fine-tuning\n",
    "\n",
    "**Next Steps:**\n",
    "1. Export to train.py script ✓\n",
    "2. Build Flask prediction service ✓\n",
    "3. Containerize with Docker\n",
    "4. Deploy to cloud (optional)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
